{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/verneh/transformers/blob/main/custom_knowledge_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwaLK4kD1Uar"
      },
      "source": [
        "#Introduction\n",
        "\n",
        "This notebook has all the code you need to create your own chatbot with custom knowledge base using GPT-3.\n",
        "\n",
        "Follow the instructions for each steps and then run the code sample. In order to run the code, you need to press \"play\" button near each code sample."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rD4Qzglp3J-h"
      },
      "source": [
        "#Download the data for your custom knowledge base\n",
        "For the demonstration purposes we are going to use ----- as our knowledge base. You can download them to your local folder from the github repository by running the code below.\n",
        "Alternatively, you can put your own custom data into the local folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cCyU-vV5Yb0",
        "outputId": "b5dc37ea-f304-45a3-d795-51afae1a49d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'context_data'...\n",
            "remote: Enumerating objects: 30, done.\u001b[K\n",
            "remote: Counting objects: 100% (2/2), done.\u001b[K\n",
            "remote: Total 30 (delta 1), reused 1 (delta 1), pack-reused 28\u001b[K\n",
            "Unpacking objects: 100% (30/30), 12.56 KiB | 584.00 KiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "! git clone https://github.com/irina1nik/context_data.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiUyHP4T2g5F"
      },
      "source": [
        "# Install the dependicies\n",
        "Run the code below to install the depencies we need for our functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LL4rxT6_W7h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e63bc2f-4268-4eb6-e869-0408ecd8159c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting llama-index==0.5.6\n",
            "  Downloading llama_index-0.5.6.tar.gz (165 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.0/165.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting dataclasses_json (from llama-index==0.5.6)\n",
            "  Downloading dataclasses_json-0.5.8-py3-none-any.whl (26 kB)\n",
            "Collecting langchain (from llama-index==0.5.6)\n",
            "  Downloading langchain-0.0.215-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama-index==0.5.6) (1.22.4)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index==0.5.6) (8.2.2)\n",
            "Collecting openai>=0.26.4 (from llama-index==0.5.6)\n",
            "  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index==0.5.6) (1.5.3)\n",
            "Collecting tiktoken (from llama-index==0.5.6)\n",
            "  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai>=0.26.4->llama-index==0.5.6) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai>=0.26.4->llama-index==0.5.6) (4.65.0)\n",
            "Collecting aiohttp (from openai>=0.26.4->llama-index==0.5.6)\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting marshmallow<4.0.0,>=3.3.0 (from dataclasses_json->llama-index==0.5.6)\n",
            "  Downloading marshmallow-3.19.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting marshmallow-enum<2.0.0,>=1.5.1 (from dataclasses_json->llama-index==0.5.6)\n",
            "  Downloading marshmallow_enum-1.5.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Collecting typing-inspect>=0.4.0 (from dataclasses_json->llama-index==0.5.6)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain->llama-index==0.5.6) (6.0)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain->llama-index==0.5.6) (2.0.16)\n",
            "Collecting async-timeout<5.0.0,>=4.0.0 (from langchain->llama-index==0.5.6)\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting langchainplus-sdk>=0.0.17 (from langchain->llama-index==0.5.6)\n",
            "  Downloading langchainplus_sdk-0.0.17-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain->llama-index==0.5.6) (2.8.4)\n",
            "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain->llama-index==0.5.6)\n",
            "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain->llama-index==0.5.6) (1.10.9)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index==0.5.6) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index==0.5.6) (2022.7.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->llama-index==0.5.6) (2022.10.31)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai>=0.26.4->llama-index==0.5.6) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai>=0.26.4->llama-index==0.5.6) (2.0.12)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->openai>=0.26.4->llama-index==0.5.6)\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0 (from aiohttp->openai>=0.26.4->llama-index==0.5.6)\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->openai>=0.26.4->llama-index==0.5.6)\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->openai>=0.26.4->llama-index==0.5.6)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses_json->llama-index==0.5.6) (23.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain->llama-index==0.5.6) (4.6.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->llama-index==0.5.6) (1.16.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai>=0.26.4->llama-index==0.5.6) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai>=0.26.4->llama-index==0.5.6) (2023.5.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai>=0.26.4->llama-index==0.5.6) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain->llama-index==0.5.6) (2.0.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.4.0->dataclasses_json->llama-index==0.5.6)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Building wheels for collected packages: llama-index\n",
            "  Building wheel for llama-index (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-index: filename=llama_index-0.5.6-py3-none-any.whl size=248136 sha256=3f036d053dbf05f24aa1d4b3b41d4a272c10196aa5e61d9018d3576cf2e53238\n",
            "  Stored in directory: /root/.cache/pip/wheels/48/9f/0a/a5d7181a1819086f8288d1becdad81de07fc874b55075dde19\n",
            "Successfully built llama-index\n",
            "Installing collected packages: mypy-extensions, multidict, marshmallow, frozenlist, async-timeout, yarl, typing-inspect, tiktoken, openapi-schema-pydantic, marshmallow-enum, langchainplus-sdk, aiosignal, dataclasses_json, aiohttp, openai, langchain, llama-index\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 dataclasses_json-0.5.8 frozenlist-1.3.3 langchain-0.0.215 langchainplus-sdk-0.0.17 llama-index-0.5.6 marshmallow-3.19.0 marshmallow-enum-1.5.1 multidict-6.0.4 mypy-extensions-1.0.0 openai-0.27.8 openapi-schema-pydantic-1.2.4 tiktoken-0.4.0 typing-inspect-0.9.0 yarl-1.9.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting langchain==0.0.148\n",
            "  Downloading langchain-0.0.148-py3-none-any.whl (636 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m636.7/636.7 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.148) (6.0)\n",
            "Collecting SQLAlchemy<2,>=1 (from langchain==0.0.148)\n",
            "  Downloading SQLAlchemy-1.4.48-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.148) (3.8.4)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.148) (4.0.2)\n",
            "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.148) (0.5.8)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.148) (2.8.4)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.148) (1.22.4)\n",
            "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.148) (1.2.4)\n",
            "Requirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.148) (1.10.9)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.148) (2.27.1)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.148) (8.2.2)\n",
            "Requirement already satisfied: tqdm>=4.48.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.148) (4.65.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.148) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.148) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.148) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.148) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.148) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.148) (1.3.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.148) (3.19.0)\n",
            "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.148) (1.5.1)\n",
            "Requirement already satisfied: typing-inspect>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.148) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain==0.0.148) (4.6.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.148) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.148) (2023.5.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.148) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<2,>=1->langchain==0.0.148) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.148) (23.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.148) (1.0.0)\n",
            "Installing collected packages: SQLAlchemy, langchain\n",
            "  Attempting uninstall: SQLAlchemy\n",
            "    Found existing installation: SQLAlchemy 2.0.16\n",
            "    Uninstalling SQLAlchemy-2.0.16:\n",
            "      Successfully uninstalled SQLAlchemy-2.0.16\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.0.215\n",
            "    Uninstalling langchain-0.0.215:\n",
            "      Successfully uninstalled langchain-0.0.215\n",
            "Successfully installed SQLAlchemy-1.4.48 langchain-0.0.148\n"
          ]
        }
      ],
      "source": [
        "!pip install llama-index==0.5.6\n",
        "!pip install langchain==0.0.148"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbuYetOy25eM"
      },
      "source": [
        "# Define the functions\n",
        "The following code defines the functions we need to construct the index and query it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "UelAqQgk_yIt"
      },
      "outputs": [],
      "source": [
        "from llama_index import SimpleDirectoryReader, GPTListIndex, readers, GPTSimpleVectorIndex, LLMPredictor, PromptHelper, ServiceContext\n",
        "from langchain import OpenAI\n",
        "import sys\n",
        "import os\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "def construct_index(directory_path):\n",
        "    # set maximum input size\n",
        "    max_input_size = 4096\n",
        "    # set number of output tokens\n",
        "    num_outputs = 3000\n",
        "    # set maximum chunk overlap\n",
        "    max_chunk_overlap = 20\n",
        "    # set chunk size limit\n",
        "    chunk_size_limit = 600\n",
        "\n",
        "    # define prompt helper\n",
        "    prompt_helper = PromptHelper(max_input_size, num_outputs, max_chunk_overlap, chunk_size_limit=chunk_size_limit)\n",
        "\n",
        "    # define LLM\n",
        "    llm_predictor = LLMPredictor(llm=OpenAI(temperature=0.5, model_name=\"text-davinci-003\", max_tokens=num_outputs))\n",
        "\n",
        "    documents = SimpleDirectoryReader(directory_path).load_data()\n",
        "\n",
        "    service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, prompt_helper=prompt_helper)\n",
        "    index = GPTSimpleVectorIndex.from_documents(documents, service_context=service_context)\n",
        "\n",
        "    index.save_to_disk('index.json')\n",
        "\n",
        "    return index\n",
        "\n",
        "def ask_ai():\n",
        "    index = GPTSimpleVectorIndex.load_from_disk('index.json')\n",
        "    while True:\n",
        "        query = input(\"What do you want to ask? \")\n",
        "        response = index.query(query)\n",
        "        display(Markdown(f\"Response: <b>{response.response}</b>\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vz1jp33jGumu"
      },
      "source": [
        "# Set OpenAI API Key\n",
        "You need an OPENAI API key to be able to run this code.\n",
        "\n",
        "If you don't have one yet, get it by [signing up](https://platform.openai.com/overview). Then click your account icon on the top right of the screen and select \"View API Keys\". Create an API key.\n",
        "\n",
        "Then run the code below and paste your API key into the text input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "RoJHE4fsAT3w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a482f2c6-218e-4f81-9cea-e120ad097c34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Paste your OpenAI key here and hit enter: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\n"
          ]
        }
      ],
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = input(\"Paste your OpenAI key here and hit enter:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVrddlAL4I_v"
      },
      "source": [
        "#Construct an index\n",
        "Now we are ready to construct the index. This will take every file in the folder 'data', split it into chunks, and embed it with OpenAI's embeddings API.\n",
        "\n",
        "**Notice:** running this code will cost you credits on your OpenAPI account ($0.02 for every 1,000 tokens). If you've just set up your account, the free credits that you have should be more than enough for this experiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "kCYTE2EqBB7O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3346db0-6b8d-4c08-f593-2122689b21ce"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<llama_index.indices.vector_store.vector_indices.GPTSimpleVectorIndex at 0x7f614cecafb0>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "construct_index(\"data/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipJ_gYxN5cWh"
      },
      "source": [
        "#Ask questions\n",
        "It's time to have fun and test our AI. Run the function that queries GPT and type your question into the input.\n",
        "\n",
        "If you've used the provided example data for your custom knowledge base, here are a few questions that you can ask:\n",
        "1. Why people cook at home? Make classification\n",
        "2. Make classification about what frustrates people about cooking?\n",
        "3. Brainstorm marketing campaign ideas for an air fryer that would appeal people that cook at home\n",
        "4. Which kitchen appliences people use most often?\n",
        "5. What people like about cooking at home?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "s_uwsPGEIGsb",
        "outputId": "3c1ffbcc-a794-4ec1-ce8e-bfa0e911e568"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What do you want to ask? What is the SLC and what is the role of Correlaid in this project?\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Response: <b>\nThe SLC is the Suriname Living Conditions Survey, a survey conducted by the government of Suriname to measure the living conditions of its citizens. Correlaid is a non-profit organization that is partnering with the SLC to provide data analysis and visualisation support for the survey. Correlaid is looking for two data analysts to help with the project, as well as a trainee to assist with the data collection and analysis.</b>"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What do you want to ask? How did Correlaid help in the survey?\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Response: <b>\nCorrelaid provided the platform for the survey to be conducted. They provided the resources and support necessary to ensure that the survey was conducted in a professional and efficient manner. They also provided the data analysis and visualisation tools needed to analyse the survey results and present them in a meaningful way.</b>"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What do you want to ask? Who were the authors from Correlaid Nederlands that helped in the Hunger Risk Mapping Suriname report and what did they do?\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Response: <b>\n\nThe authors from CorrelAidxNL that helped in the Hunger Risk Mapping Suriname report were:\n\n- Jeroen van der Velden: Lead researcher and data analyst, responsible for calculating food expenditure deficits according to energy sufficient, nutrient adequate, and healthy diets, and converting all results into 2020 US dollars.\n- Joram van der Heide: Lead data analyst, responsible for analyzing household consumption data and estimating necessary non-food expenses based on the 2016 Survey of Living Conditions.\n- Jelmer van der Heide: Lead data analyst, responsible for scaling population numbers according to 2020 total population number for Suriname and capping deficits by food cost.\n- Jelle van der Heide: Lead data analyst, responsible for providing raw data and plots of food expenditure deficits.</b>"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What do you want to ask? ho were the authors from Correlaid Nederlands for the Hunger Risk mapping Suriname Report?\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Response: <b>\n\nThe authors from CorrelAid Nederlands for the Hunger Risk mapping Suriname Report were a team of data scientists, researchers, and analysts. They used a combination of quantitative and qualitative data sources to analyze the food security situation in Suriname. They used open data sources such as the FAO, USDA, Comtrade, ILO, Inter-American Development Bank, Algemeen Bureau Statistiek Suriname, Centrale Bank van Suriname, CARICOM, International Red Cross, and UN, UNDP. They used this data to analyze food availability, food prices, household incomes, affordability of nutritious diet, floods, and local products and existing food aid. They then estimated food expenditure deficits for an energy sufficient diet, a nutrient adequate diet, and a healthy diet, based on household consumption data and estimates of necessary non-food expenses. All results were converted into 2020 US dollars and presented in a report on 22 December 2021.</b>"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What do you want to ask? Who exactly were the authors from Correlaid Nederlands for the Hunger Risk mapping Suriname Report?\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Response: <b>\n\nThe authors of the Hunger Risk Mapping Suriname Report from CorrelAidxNL were a team of data scientists, researchers, and analysts. The team included: \n\n- Dr. Jeroen van der Heijden, Lead Data Scientist\n- Dr. Marije van der Heijden, Lead Researcher\n- Dr. Jeroen van der Heijden, Lead Analyst\n- Dr. Marije van der Heijden, Lead Writer\n- Dr. Jeroen van der Heijden, Lead Editor\n- Dr. Marije van der Heijden, Lead Designer\n- Dr. Jeroen van der Heijden, Lead Programmer\n- Dr. Marije van der Heijden, Lead Data Visualizer\n\nThe team used data from the 2016 Survey of Living Conditions to calculate food expenditure deficits according to three diets (energy sufficient, nutrient adequate, and healthy) and converted the results into 2020 US dollars. The raw data and plots of the results can be found at the following links: \n\nEnergy Sufficient Diet\nRaw data: https://github.com/CorrelAidxNL/slc-hunger-risk/blob/main/</b>"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What do you want to ask? Can you summarize the SLC Presentation and mention who its authors are?\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Response: <b>\nThe SLC Presentation is a project by CorrelAid, a non-profit organization that works to improve the lives of people in need through data science. The project aims to provide evidence-based data to the Suriname government to help them better understand and address the issue of food insecurity in the country. The authors of the presentation are Valerie Habbel, Patryk Kubiczek, Sandra Morgenstern, David Jankoski, Maria Bader, and Liza Olenderek. All of them have experience in data analysis, software development, and research, and are motivated to use their skills to contribute to the project.</b>"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What do you want to ask? Can you summarize the project goals for SLC?\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Response: <b>\nThe project goals for SLC are to use data analysis and visualization to support their application for emergency food aid in Suriname. They will be conducting interviews with key organizations in Suriname and researching open data sources to test the claim that there is a risk for food shortage in Suriname, and if so, which locations/regions will be affected the most. They will also be looking for data on food insecurity, such as food production, imports and exports, poverty, smallholder/subsistence farming, access to food (roads/transport), and other factors that may contribute to food insecurity.</b>"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What do you want to ask? The major issue for food insecurity is reduced household incomes combined with increasing food prices. Surinamese have less income to spend on food that is increasingly expensive.   Smart Living Connection is a foundation which aims to help the socioeconomic and cultural development of Curaçao, Suriname, Aruba and Sint-Maarten. It has been involved in the design, securement of financing, and implementation of projects to stimulate economic growth, combat poverty, and safeguard cultural heritage.  Last year a report was produced pro bono publico by CorrelAidxNL volunteers in collaboration with Smart Lifestyle Connection.  They were involved in Quick Analyzed Data project on Food Security in the Caribbean Part of the Kingdom. The corona pandemic and lockdowns paralyzed the tourism industry on the islands, causing huge job losses. The result was a lack of income and food. With this project, and the resulting data analysis document, SLC was successful in securing emergency food aid for the islands.  Currently, another large food shortage is expected, this time in Suriname, mainland of South America. Help has been requested, including from the IMF, but it is expected to only come through in six months or more. Already political, economic, and financial reforms by a new government are on their way, but it is important to convince emergency aid institutions in the Netherlands and EU of the severity and urgency of the issue.  SLC has contacts in the Dutch Ministry of Foreign Affairs, and development organizations. SLC needs to back up its story and convince these institutions of the severity of this issue and ask for their support in providing (the finances for) emergency food aid. SLC will be conducting interviews with people from key organizations in Suriname over the next few weeks, such as farming cooperatives, parliamentarians, the Emergency Committee at the ministry of Agriculture, UN representatives in Suriname, and NGO fieldworkers.  Using different open data sources, you will look for data on food insecurity, e.g. by combining data on food production, imports and exports, poverty, smallholder/subsistence farming, access to food (roads/transport), and other factors that may contribute to food insecurity. Data sources may include UNDP, EU, WorldBank, Suriname’s bureau of statistics, HumData, and others. Summarize this as an informative not saleslike post for LinkedIn\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Response: <b>\nSmart Living Connection is looking for two data scientists/analysts to join their team in a project to help combat food insecurity in Suriname. The project will involve researching open data sources to analyze and visualize data on food security in Suriname to support SLC’s application for help. The data will be used to back up SLC’s story and convince emergency aid institutions in the Netherlands and EU of the severity and urgency of the issue. The project will start on 11th October 2021 and end on 22nd December 2021, with an expected time effort of 4 hours a week. If you have experience in Python or R for analysis and a passion for humanitarian goals, apply now before the 8th October 2021 deadline.</b>"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What do you want to ask? The major issue for food insecurity is reduced household incomes combined with increasing food prices. Surinamese have less income to spend on food that is increasingly expensive.  Currently, another large food shortage is expected, this time in Suriname, mainland of South America. Help has been requested, including from the IMF, but it is expected to only come through in six months or more. Already political, economic, and financial reforms by a new government are on their way, but it is important to convince emergency aid institutions in the Netherlands and EU of the severity and urgency of the issue.  Smart Lifestyle Connection is a foundation which aims to help the socioeconomic and cultural development of Curaçao, Suriname, Aruba and Sint-Maarten. It has been involved in the design, securement of financing, and implementation of projects to stimulate economic growth, combat poverty, and safeguard cultural heritage. Last year, they were involved in the Quick Analyzed Data project on Food Security in the Caribbean Part of the Kingdom. The corona pandemic and lockdowns paralyzed the tourism industry on the islands, causing huge job losses. The result was a lack of income and food.   SLC has contacts in the Dutch Ministry of Foreign Affairs, and development organizations. SLC needs to back up its story and convince these institutions of the severity of this issue and ask for their support in providing (the finances for) emergency food aid. SLC will be conducting interviews with people from key organizations in Suriname over the next few weeks, such as farming cooperatives, parliamentarians, the Emergency Committee at the ministry of Agriculture, UN representatives in Suriname, and NGO fieldworkers.  In collaboration with Smart Lifestyle Connection, a report on Hunger Risk Mapping Suriname was produced pro bono publico by CorrelAidxNL volunteers, namely, Maria Bader, Maya Bogers, Valerie Habbel, David Jankoski, Patryk Kubiczek, Liza Olenderek & Sander Rurup in order to test the claims of risk for food shortage in Suriname.   Using different open data sources,  they searched for data on food insecurity, e.g. by combining data on food production, imports and exports, poverty, smallholder/subsistence farming, access to food (roads/transport), and other factors that may contribute to food insecurity.   Thanks to this SLC project and the findings from our volunteers report, SLC was successful in securing emergency food aid for the islands. Rewrite this as a blog post.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Response: <b>\nThe Caribbean islands of Curaçao, Suriname, Aruba, and Sint-Maarten have been hit hard by the coronavirus pandemic. With the tourism industry paralyzed, many people have lost their jobs and are struggling to make ends meet. Smart Lifestyle Connection (SLC), a foundation that works to promote socioeconomic and cultural development in the region, has been working to secure emergency food aid for the islands. \n\nRecently, SLC has noted signs of hunger risk in Suriname and requested a quickscan on hunger risk with available open data. In response, CorrelAidxNL volunteers produced a report on Hunger Risk Mapping Suriname pro bono publico. The report used different open data sources to search for data on food insecurity, such as food production, imports and exports, poverty, smallholder/subsistence farming, access to food (roads/transport), and other factors that may contribute to food insecurity. The report found that the major issue for food insecurity is reduced household incomes combined with increasing food prices. Surinamese have less income to spend on food that is increasingly expensive. In addition, the floods in 2021 have resulted in loss of household assets and flooding of agricultural fields, and</b>"
          },
          "metadata": {}
        }
      ],
      "source": [
        "ask_ai()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6ATR2jSOHaes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_5ikolfJHZX1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}